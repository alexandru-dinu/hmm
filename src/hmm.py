# coding: utf-8

# Hidden Markov Models
# skel by Tudor Berariu (tudor.berariu@gmail.com), 2018


import numpy as np

from grids import COLORS, GRIDS
from utils import sample


# The problem: *The Climber Robot*


# ================== Extracting the HMM parameters ==================
# Given a `Grid` object, build the three matrices of parameters of HMM := <pi, A, B>

# Uniform distribution
def get_initial_distribution(grid):
    N = grid.states_no
    return np.ones(N) / N


# Transition probability matrix
def get_transition_probabilities(grid):
    H, W = grid.shape
    N = H * W
    A = np.zeros((N, N))

    states = [(i, j) for i in range(H) for j in range(W)]

    for si in states:
        for sj in states:
            si_next = dict(grid.get_neighbours(si))

            if sj not in si_next.keys():
                A[states.index(si), states.index(sj)] = 0
            else:
                A[states.index(si), states.index(sj)] = si_next[sj]

    return A


# Emission probability matrix
def get_emission_probabilities(grid, num_possible_obs=len(COLORS)):
    H, W = grid.shape
    N = grid.states_no
    B = np.zeros((H * W, num_possible_obs))

    states = [(i, j) for i in range(H) for j in range(W)]

    for si in states:
        for ck in np.arange(num_possible_obs):
            clrs = dict(grid.get_colors(si))
            B[states.index(si), ck] = clrs[ck]

    return B


# Given a model (a `Grid`), return a sequence of observations and the corresponding states.
def get_sequence(grid, length):
    H, W = grid.shape

    states, observations = [], []
    for t in range(length):
        # choose a random init state
        if t == 0:
            state = np.random.randint(H), np.random.randint(W)
        else:
            state = sample(grid.get_neighbours(state))
        o = sample(grid.get_colors(state))
        states.append(state)
        observations.append(o)

    return np.array(observations), states


# ====================== Evaluation =======================
# We'll now evaluate the probability that a given sequence of observations was generated by a given model.
# We will look at a sequence and see if we can figure out which grid generated it.


# Compute the probability that a given sequence comes from a given model
def forward(grid, observations, hmm=None):
    N = grid.states_no
    T = len(observations)
    alpha = np.zeros((T, N))

    if hmm is None:
        pi = get_initial_distribution(grid)
        A = get_transition_probabilities(grid)
        B = get_emission_probabilities(grid, num_possible_obs=len(COLORS))
    else:
        pi, A, B = hmm

    # alpha_0i = pi_i * B_i0
    alpha[0, :] = pi * B[:, observations[0]]

    # alpha_ti = sum_j(alpha_t-1,j * Aji) * B_it
    for t in range(1, T):
        alpha[t, :] = np.sum(alpha[t - 1, :] * np.transpose(A), axis=1) * B[:, observations[t]]

    # p_obs = p(obs | hmm)
    p_obs = alpha[-1, :].sum()

    return p_obs, alpha


def backward(grid, observations, hmm=None):
    N = grid.states_no
    T = len(observations)
    beta = np.zeros((T, N))

    if hmm is None:
        pi = get_initial_distribution(grid)
        A = get_transition_probabilities(grid)
        B = get_emission_probabilities(grid, num_possible_obs=len(COLORS))
    else:
        pi, A, B = hmm

    # beta_T-1,i = 1
    beta[T - 1, :] = 1

    # beta_ti = sum_j(beta_t+1,j * A_ij * B_j,t+1)
    for t in range(T - 2, -1, -1):
        beta[t, :] = np.sum(beta[t + 1, :] * A * B[:, observations[t + 1]], axis=1)

    # p_obs = p(obs | hmm)
    p_obs = np.sum(pi * B[:, observations[0]] * beta[0, :])

    return p_obs, beta


# Decoding: compute the most probable sequence of states that generated the observations
def viterbi(grid, observations, hmm=None):
    N = grid.states_no
    H, W = grid.shape
    T = len(observations)
    delta = np.zeros((T, N))
    states = np.zeros(T, dtype=int)

    if hmm is None:
        pi = get_initial_distribution(grid)
        A = get_transition_probabilities(grid)
        B = get_emission_probabilities(grid, num_possible_obs=len(COLORS))
    else:
        pi, A, B = hmm

    # t == 0
    delta[0, :] = pi * B[:, observations[0]]

    for t in range(1, T):
        delta[t, :] = np.max(delta[t - 1, :] * np.transpose(A), axis=1) * B[:, observations[t]]

    states[T - 1] = np.argmax(delta[T - 1, :])
    for t in range(T - 2, -1, -1):
        states[t] = np.argmax(delta[t, :] * A[:, states[t + 1]])

    return [(s // W, s % W) for s in states], delta


def init(grid, num_possible_obs):
    N = grid.states_no
    M = num_possible_obs

    # initial distribution
    pi = np.ones(N) / N

    # initial transition probabilities
    # A = get_transition_probabilities(grid)
    A = np.zeros((N, N))
    for i in range(N):
        A[i, :] = np.random.dirichlet(np.ones(N))
        # x = np.random.random(N)
        # A[i, :] = x / x.sum()

    # initial emission probabilities
    # B = get_emission_probabilities(grid, num_possible_obs=len(COLORS))
    B = np.ones((N, M)) / M

    return pi, A, B


def expectation(grid, samples, hmm):
    pi, A, B = hmm
    N = grid.states_no
    T = len(observations)

    xi = np.zeros((T - 1, N, N))

    # p_obs = p(obs | theta)
    p_obs, alpha = forward(grid, observations, hmm)
    p_obs_b, beta = backward(grid, observations, hmm)
    p_obs_s = np.sum(alpha * beta, axis=1)

    assert np.allclose(p_obs, p_obs_b, equal_nan=True)
    assert np.allclose(np.repeat(p_obs, T), p_obs_s, equal_nan=True)

    # gamma_ti = (alpha_ti * beta_ti) / p(obs | theta) (size = T x N)
    gamma = alpha * beta / p_obs

    # xi_tij = (alpha_ti * A_ij * beta_t+1,j * B_j,t+1) / p(obs | theta) (size = T-1 x N x N)
    for t in range(T - 1):
        xi[t, :, :] = np.tile(alpha[t, :], (N, 1)).transpose() * A * B[:, observations[t + 1]] * beta[t + 1, :] / p_obs

    return gamma, xi, np.log(p_obs)


def maximization(grid, samples, num_possible_observations, gamma, xi, hmm):
    pi, A, B = hmm
    N = grid.states_no
    M = num_possible_observations

    # update pi
    pi = gamma[0, :]

    # # gamma_ti = sum_j(xi_tij)
    assert np.allclose(gamma[:-1, :], np.sum(xi, axis=2), equal_nan=True)

    # update: A <- sum_t(xi_tij) / sum_t(gamma_ti) | all t's but last one
    for i in range(N):
        for j in range(N):
            A[i, j] = np.sum(xi[:-1, i, j]) / np.sum(gamma[:-1, i])

    # update: B <- sum_t(1(y_t==k) * gamma_ti) / sum_t(gamma_ti)
    for i in range(N):
        for k in range(M):
            B[i, k] = np.sum((observations == k) * gamma[:, i]) / np.sum(gamma[:, i])

    return pi, A, B


def baum_welch(grid, samples, num_possible_obs):

    pi, A, B = init(grid, num_possible_obs)

    _, _, old_logp = expectation(grid, samples, hmm=(pi, A, B))

    for j in range(100):
        pi, A, B = maximization(grid, samples, len(COLORS), gamma, xi, hmm=(pi, A, B))
        gamma, xi, logp = expectation(grid, samples, hmm=(pi, A, B))

        diff = np.abs(logp, old_logp)
        print("diff = %.8f" % diff)

        return pi, A, B



# -----
def maximiz(grid, samples, alphas, betas, ps, hmm):
    pi, A, B = hmm

    N = grid.states_no
    L = len(samples)
    M = len(COLORS)

    gammas, xis = [], []
    for l in range(L):
        T = len(samples[l])

        xi = np.zeros((T-1, N, N))
        for t in range(T - 1):
            xi[t, :, :] = np.tile(alphas[l][t, :], (N, 1)).transpose() * A * B[:, samples[l][t + 1]] * betas[l][t + 1, :]

        xis.append(xi / ps[l])
        gammas.append(alphas[l] * betas[l] / ps[l])

    # update pi
    pi = np.zeros(N)
    for i in range(N):
        x, y = 0, 0
        for l in range(L):
            x += gammas[l][0, i]
            y += np.sum(gammas[l][0, :])
        pi[i] = x / y

    # update A
    A = np.zeros((N, N))
    for i in range(N):
        for j in range(N):
            x, y = 0, 0

            for l in range(L):
                x += np.sum(xis[l][:-1, i, j])
                y += np.sum(gammas[l][:-1, i])

            A[i, j] = x / y

    # update B
    for i in range(N):
        for k in range(M):
            x, y = 0, 0

            for l in range(L):
                x += np.sum((samples[l] == k) * gammas[l][:, i])
                y += np.sum(gammas[l][:, i])

            B[i, k] = x / y

    return pi, A, B




def bw(grid, samples):
    # pi, A, B = init(grid, len(COLORS))
    pi = get_initial_distribution(grid)
    A = get_transition_probabilities(grid)
    B = get_emission_probabilities(grid)

    old_logp = 0

    alphas, betas, ps = [], [], []
    for l in range(len(samples)):
        p, a = forward(grid, samples[l], hmm=(pi, A, B))
        _, b = backward(grid, samples[l], hmm=(pi, A, B))

        ps += [p]
        alphas += [a]
        betas += [b]
    logp = np.mean(np.log(ps))

    it = 0
    while True: #np.abs(logp - old_logp) > 1e-5:
        it += 1
        print("Iter [%5d], logp = %.8f, old_logp = %.8f, diff = [%.8f]" %
        (it, logp, old_logp, logp - old_logp))

        old_logp = logp

        pi, A, B = maximiz(grid, samples, alphas, betas, ps, hmm=(pi, A, B))

        alphas, betas, ps = [], [], []
        for l in range(len(samples)):
            p, a = forward(grid, samples[l], hmm=(pi, A, B))
            _, b = backward(grid, samples[l], hmm=(pi, A, B))

            ps += [p]
            alphas += [a]
            betas += [b]

        logp = np.mean(np.log(ps))

    return pi, A, B
# ---



if __name__ == '__main__':
    np.set_printoptions(suppress=True)
    import sys

    grid = GRIDS[1]

    pi_true = get_initial_distribution(grid)
    A_true = get_transition_probabilities(grid)
    B_true = get_emission_probabilities(grid, num_possible_obs=len(COLORS))

    num_samples = int(sys.argv[1])
    samples = [None] * num_samples

    for s in range(num_samples):
        length = np.random.randint(5, 11)
        obs, _ = get_sequence(grid, length)
        # p, _ = forward(grid, obs, hmm=(pi_true, A_true, B_true))
        samples[s] = obs

    # pi, A, B = baum_welch(grid, samples, num_possible_obs=len(COLORS))
    pi, A, B = bw(grid, samples)
