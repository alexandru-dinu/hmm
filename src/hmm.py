# coding: utf-8

# Hidden Markov Models
# skel by Tudor Berariu (tudor.berariu@gmail.com), 2018


import sys

import numpy as np

from grids import COLORS, GRIDS
from utils import sample
import matplotlib.pyplot as plt


# The problem: *The Climber Robot*


# ================== Extracting the HMM parameters ==================
# Given a `Grid` object, build the three matrices of parameters of HMM := <pi, A, B>

# Uniform distribution
def get_initial_distribution(grid):
    N = grid.states_no
    return np.ones(N) / N


# Transition probability matrix
def get_transition_probabilities(grid):
    H, W = grid.shape
    N = H * W
    A = np.zeros((N, N))

    states = [(i, j) for i in range(H) for j in range(W)]

    for si in states:
        for sj in states:
            si_next = dict(grid.get_neighbours(si))

            if sj not in si_next.keys():
                A[states.index(si), states.index(sj)] = 0
            else:
                A[states.index(si), states.index(sj)] = si_next[sj]

    return A


# Emission probability matrix
def get_emission_probabilities(grid, num_possible_obs=len(COLORS)):
    H, W = grid.shape
    N = grid.states_no
    B = np.zeros((H * W, num_possible_obs))

    states = [(i, j) for i in range(H) for j in range(W)]

    for si in states:
        for ck in np.arange(num_possible_obs):
            clrs = dict(grid.get_colors(si))
            B[states.index(si), ck] = clrs[ck]

    return B


# Given a model (a `Grid`), return a sequence of observations and the corresponding states.
def get_sequence(grid, length):
    H, W = grid.shape

    states, observations = [], []
    for t in range(length):
        # choose a random init state
        if t == 0:
            state = np.random.randint(H), np.random.randint(W)
        else:
            state = sample(grid.get_neighbours(state))
        o = sample(grid.get_colors(state))
        states.append(state)
        observations.append(o)

    return np.array(observations), states


# ====================== Evaluation =======================
# We'll now evaluate the probability that a given sequence of observations was generated by a given model.
# We will look at a sequence and see if we can figure out which grid generated it.


# Compute the probability that a given sequence comes from a given model
def forward(grid, observations, hmm=None):
    N = grid.states_no
    T = len(observations)
    alpha = np.zeros((T, N))

    if hmm is None:
        pi = get_initial_distribution(grid)
        A = get_transition_probabilities(grid)
        B = get_emission_probabilities(grid, num_possible_obs=len(COLORS))
    else:
        pi, A, B = hmm

    # alpha_0i = pi_i * B_i0
    alpha[0, :] = pi * B[:, observations[0]]

    # alpha_ti = sum_j(alpha_t-1,j * Aji) * B_it
    for t in range(1, T):
        alpha[t, :] = np.sum(alpha[t - 1, :] * np.transpose(A), axis=1) * B[:, observations[t]]

    # p_obs = p(obs | hmm)
    p_obs = alpha[-1, :].sum()

    return p_obs, alpha


def backward(grid, observations, hmm=None):
    N = grid.states_no
    T = len(observations)
    beta = np.zeros((T, N))

    if hmm is None:
        pi = get_initial_distribution(grid)
        A = get_transition_probabilities(grid)
        B = get_emission_probabilities(grid, num_possible_obs=len(COLORS))
    else:
        pi, A, B = hmm

    # beta_T-1,i = 1
    beta[T - 1, :] = 1

    # beta_ti = sum_j(beta_t+1,j * A_ij * B_j,t+1)
    for t in range(T - 2, -1, -1):
        beta[t, :] = np.sum(beta[t + 1, :] * A * B[:, observations[t + 1]], axis=1)

    # p_obs = p(obs | hmm)
    p_obs = np.sum(pi * B[:, observations[0]] * beta[0, :])

    return p_obs, beta


# Decoding: compute the most probable sequence of states that generated the observations
def viterbi(grid, observations, hmm=None):
    N = grid.states_no
    H, W = grid.shape
    T = len(observations)
    delta = np.zeros((T, N))
    states = np.zeros(T, dtype=int)

    if hmm is None:
        pi = get_initial_distribution(grid)
        A = get_transition_probabilities(grid)
        B = get_emission_probabilities(grid, num_possible_obs=len(COLORS))
    else:
        pi, A, B = hmm

    # t == 0
    delta[0, :] = pi * B[:, observations[0]]

    for t in range(1, T):
        delta[t, :] = np.max(delta[t - 1, :] * np.transpose(A), axis=1) * B[:, observations[t]]

    states[T - 1] = np.argmax(delta[T - 1, :])
    for t in range(T - 2, -1, -1):
        states[t] = np.argmax(delta[t, :] * A[:, states[t + 1]])

    return [(s // W, s % W) for s in states], delta


def init(grid, num_possible_obs):
    N = grid.states_no
    M = num_possible_obs

    # initial distribution
    pi = np.ones(N) / N

    # initial transition probabilities
    # A = get_transition_probabilities(grid)
    A = np.zeros((N, N))
    for i in range(N):
        x = np.random.random(N)
        A[i, :] = x / x.sum()

    # initial emission probabilities
    # B = get_emission_probabilities(grid, num_possible_obs=len(COLORS))
    B = np.ones((N, M)) / M

    return pi, A, B


def expectation(grid, samples, hmm):
    pi, A, B = hmm
    L = len(samples)
    N = grid.states_no

    logp = 0
    gammas, xis = [], []

    for l in range(L):
        p, alpha = forward(grid, samples[l], hmm=(pi, A, B))
        _, beta = backward(grid, samples[l], hmm=(pi, A, B))

        T = len(samples[l])

        xi = np.zeros((T - 1, N, N))
        for t in range(T - 1):
            xi[t, :, :] = np.tile(alpha[t, :], (N, 1)).transpose() * A * B[:, samples[l][t + 1]] * beta[t + 1, :]

        xis += [xi / p]
        gammas += [alpha * beta / p]

        logp += np.log(p)

    return gammas, xis, logp / L


def maximization(grid, samples, gammas, xis, hmm):
    pi, A, B = hmm

    N = grid.states_no
    L = len(samples)
    M = len(COLORS)

    # update pi
    pi = np.zeros(N)
    for i in range(N):
        pi[i] = np.sum([gammas[l][0, i] for l in range(L)]) / L

    # update A
    A = np.zeros((N, N))
    for i in range(N):
        for j in range(N):
            x, y = 0, 0

            for l in range(L):
                x += np.sum(xis[l][:-1, i, j])
                y += np.sum(gammas[l][:-1, i])

            A[i, j] = x / y

    # update B
    for i in range(N):
        for k in range(M):
            x, y = 0, 0

            for l in range(L):
                x += np.sum((samples[l] == k) * gammas[l][:, i])
                y += np.sum(gammas[l][:, i])

            B[i, k] = x / y

    return pi, A, B


def baum_welch(grid, samples):
    pi, A, B = init(grid, len(COLORS))
    # pi = get_initial_distribution(grid)
    # A = get_transition_probabilities(grid)
    # B = get_emission_probabilities(grid)

    gammas, xis, logp = expectation(grid, samples, hmm=(pi, A, B))

    logps = [0, logp]

    it = 0
    while True:
        it += 1

        print("Iter [%5d], logp = %.8f, old_logp = %.8f, diff = [%.8f]" % (it, logps[-1], logps[-2], logps[-1] - logps[-2]))

        if it % 100 == 0:
            plt.plot(np.arange(it), logps[1:], color='b', lw=0.5)
            plt.show()

        pi, A, B = maximization(grid, samples, gammas, xis, hmm=(pi, A, B))
        gammas, xis, logp = expectation(grid, samples, hmm=(pi, A, B))

        logps += [logp]

    return pi, A, B


# ---


def main(args):
    np.set_printoptions(suppress=True)

    grid = GRIDS[int(args[0])]

    pi_true = get_initial_distribution(grid)
    A_true = get_transition_probabilities(grid)
    B_true = get_emission_probabilities(grid)

    num_samples = int(args[1])
    samples = []

    for s in range(num_samples):
        length = np.random.randint(5, 11)
        obs, _ = get_sequence(grid, length)
        samples += [obs]

    pi, A, B = baum_welch(grid, samples)


if __name__ == '__main__':
    main(sys.argv[1:])
